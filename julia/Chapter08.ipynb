{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LaTeXStrings\n",
    "default(markersize=3, linewidth=1.5)\n",
    "using Images, TestImages\n",
    "using MatrixDepot, JLD\n",
    "using LinearAlgebra\n",
    "using SparseArrays, IncompleteLU, Arpack\n",
    "using IterativeSolvers, LinearMaps, Preconditioners\n",
    "#include(\"FNC.jl\");\n",
    "include(\"functions/chapter02.jl\")\n",
    "include(\"functions/chapter08.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the adjacency matrix of a graph with 2790 nodes. Each node is a web page referring to Roswell, NM, and the edges represent links between web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = load(\"roswelladj.jld\")       # get from the book's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = vars[\"i\"], vars[\"j\"]\n",
    "n = maximum([i; j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sparse(i,j,ones(size(i)),n,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may define the density of $\\mathbf{A}$ as the number of nonzeros divided by the total number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nnz(A) / n^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the storage space needed for the sparse $\\mathbf{A}$ with the space needed for its dense or full counterpart. This ratio can never be as small as the density of nonzeros, because of the need to store locations as well as data. However, it's still quite small here, even though the matrix is not really large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = Matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(r\"A\")                       # to see memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(r\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageratio = 111e3/59e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-vector products are also much faster using the sparse form, because operations with structural zeros are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(n)\n",
    "b = A*x\n",
    "@elapsed for i = 1:200; A*x; end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = F*x\n",
    "@elapsed for i = 1:200; F*x; end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the sparse storage format is column-oriented. Operations on rows may take a lot longer than similar ones on columns. (Note: Such behavior is dramatic here for MATLAB, but not Julia.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = A[:,1000]\n",
    "\n",
    "r = v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"time for replacing columns:\")\n",
    "for i = 1:n; A[:,i]=v; end     # run once to improve timing accuracy\n",
    "@elapsed for i = 1:n; A[:,i]=v; end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"time for replacing rows:\")\n",
    "for i = 1:n; A[i,:]=r; end     # run once to improve timing accuracy\n",
    "@elapsed for i = 1:n; A[i,:]=r; end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the adjacency matrix of a graph representing a \"small world\" network featuring connections to neighbors and a small number of strangers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = matrixdepot(\"smallworld\",100,4,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy(A,color=:black,size=(600,600),m=1.5,colorbar=false)\n",
    "xlims!(-5,105); ylims!(-5,105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of vertex pairs connected by a path of length $k>1$ grows with $k$, as can be seen here for $k=4$. (This would be \"four degrees of separation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy(A^2,color=:bluesreds,size=(600,600),m=1.5)\n",
    "xlims!(-5,105); ylims!(-5,105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a matrix with both lower and upper bandwidth equal to one. Such a matrix is called *tridiagonal*. The `spdiagm` command creates a sparse matrix given its diagonal elements. The main or central diagonal is numbered zero, above and to the right of that is positive, and below and to the left is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50;\n",
    "A = spdiagm(-3=>fill(n,n-3),0=>ones(n),1=>-(1:n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix( A[1:7,1:7] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without pivoting, the LU factors have the same lower and upper bandwidth as the orignal matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,U = lufact(A)\n",
    "spy(A,layout=(1,3),subplot=1,markersize=2,title=\"A\",color=:bluesreds,colorbar=false,size=(900,300))\n",
    "spy!(sparse(L),subplot=2,markersize=2,title=\"L\",color=:bluesreds,colorbar=false)\n",
    "spy!(sparse(U),subplot=3,markersize=2,title=\"U\",color=:bluesreds,colorbar=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we introduce row pivoting, bandedness may be expanded or destroyed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = lu(A)\n",
    "spy(A,layout=(1,3),subplot=1,markersize=2,title=\"A\",color=:bluesreds,colorbar=false,size=(900,300))\n",
    "spy!(sparse(fact.L),subplot=2,markersize=2,title=\"L\",color=:bluesreds,colorbar=false)\n",
    "spy!(sparse(fact.U),subplot=3,markersize=2,title=\"U\",color=:bluesreds,colorbar=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generates a random sparse matrix with prescribed eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "density = 1.23e-3\n",
    "lambda = @. 1/(1:n)\n",
    "A = sprandsym(n,density,lambda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy(A,title=\"Sparse symmetric matrix\",color=:bluesreds,colorbar=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda,V = eigs(A,nev=5,which=:LM)    # largest magnitude\n",
    "lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda,V = eigs(A,nev=5,sigma=0)    # closest to zero\n",
    "1 ./ lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling of time to solve a sparse linear system is not easy to predict unless you have some more information about the matrix (such as bandedness). But it will typically be a great deal faster than the dense or full matrix case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = @. 1/(1:n);  b = A*x;\n",
    "@time sparse_err = norm(x - A\\b)\n",
    "@time sparse_err = norm(x - A\\b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Matrix(A)  # convert to regular matrix\n",
    "x = @. 1/(1:n);  b = A*x;\n",
    "@time dense_err = norm(x - A\\b)\n",
    "@time dense_err = norm(x - A\\b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.108273/0.000476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_err, dense_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we let $\\mathbf{A}$ be a $5\\times 5$ matrix. We also choose a random 5-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(1.:9.,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(A,dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A./sum(A,dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(A,dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying matrix-vector multiplication once doesn't do anything recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = A*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the multiplication still doesn't do anything obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = A*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we keep repeating the matrix-vector multiplication, something remarkable happens: $\\mathbf{A}\\mathbf{x}\\approx \\mathbf{x}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j = 1:8;  x = A*x;  end\n",
    "[x A*x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(x - A*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to occur regardless of the starting value of $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(5)\n",
    "for j = 1:10;  x = A*x;  end   # computing A^10*x\n",
    "[x A*x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(x - A*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a $5\\times 5$ matrix with prescribed eigenvalues, then apply the power iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = [1,-0.75,0.6,-0.4,0]\n",
    "A = triu(ones(5,5),1) + diagm(lambda)   # triangular matrix, eigs on diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the power iteration 60 times. The best estimate of the dominant eigenvalue is the last entry of `gamma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma,x = poweriter(A,60)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval = gamma[end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check linear convergence using a log-linear plot of the error. We use our best estimate in order to compute the error at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = eigval .- gamma\n",
    "plot(0:59,abs.(err),m=:o,label=\"\", \n",
    "    title=\"Convergence of power iteration\",\n",
    "    xlabel=L\"k\",yaxis=(L\"|\\lambda_1 - \\gamma_k|\",:log10,[1e-9,1e1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend is clearly a straight line asymptotically. We can get a refined estimate of the error reduction in each step by using the exact eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show theory = lambda[2]/lambda[1];\n",
    "@show observed = err[40]/err[39];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error is supposed to change sign on each iteration. An effect of these alternating signs is that estimates oscillate around the exact value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma[36:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambar = (gamma[60]+gamma[59])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a $5\\times 5$ triangular matrix with prescribed eigenvalues on its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = [1,-0.75,0.6,-0.4,0]\n",
    "A = triu(ones(5,5),1) + diagm(lambda)   # triangular matrix, eigs on diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run inverse iteration with the shift $s=0.7$ and take the final estimate as our ``exact'' answer to observe the convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma,x = inviter(A,0.7,30)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval = gamma[end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the eigenvalue that was found is the one closest to $0.7$. The convergence is again linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = eigval .- gamma\n",
    "plot(0:29,abs.(err),m=:o,label=\"\", \n",
    "    title=\"Convergence of inverse iteration\",\n",
    "    xlabel=L\"k\",yaxis=(L\"|\\lambda_3 - \\gamma_k|\",:log10,[1e-16,1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed linear convergence rate is found from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_rate = err[22]/err[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the numbering of this example, the eigenvalue closest to $s=0.7$ is $\\lambda_3$ and the next-closest is $\\lambda_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda .- 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_rate = (lambda[3]-0.7) / (lambda[1]-0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = [1,-0.75,0.6,-0.4,0]\n",
    "A = triu(ones(5,5),1) + diagm(lambda)   # triangular matrix, eigs on diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a shift $s=0.7$, which is closest to the eigenvalue 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.7\n",
    "x = ones(5)\n",
    "y = (A-s*I)\\x\n",
    "gamma = x[1]/y[1] + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result is not yet any closer to the targeted $0.6$. But we proceed (without being too picky about normalization here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = gamma\n",
    "x = y/y[1]\n",
    "y = (A-s*I)\\x\n",
    "gamma = x[1]/y[1] + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not much apparent progress. However, in just a few more iterations the results are dramatically better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k = 1:4\n",
    "    s = gamma  \n",
    "    x = y/y[1]\n",
    "    y = (A-s*I)\\x  \n",
    "    gamma = x[1]/y[1] + s\n",
    "    @show gamma\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a triangular matrix with known eigenvalues and a random vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = @. 10 + (1:100)\n",
    "A = triu(rand(100,100),1) + diagm(lambda)\n",
    "b = rand(100);\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build up the first ten Krylov matrices iteratively, using renormalization after each matrix-vector multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Km = [b/norm(b) zeros(100,29)]\n",
    "for m = 1:29      \n",
    "    v = A*Km[:,m]\n",
    "    Km[:,m+1] = v/norm(v)\n",
    "end\n",
    "Km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we solve a least squares problem for Krylov matrices of increasing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = zeros(30)\n",
    "for m = 1:30  \n",
    "    z = (A*Km[:,1:m])\\b  # Solve the least-squares problems\n",
    "    x = Km[:,1:m]*z\n",
    "    resid[m] = norm(b-A*x)\n",
    "end\n",
    "resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear system approximations show smooth linear convergence at first, but the convergence stagnates after only a few digits have been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(0:29,resid,m=:o,\n",
    "    xaxis=(L\"m\"),yaxis=(:log10,L\"\\| b-Ax_m \\|\"), \n",
    "    title=\"Residual for linear systems\",leg=:none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate a few steps of the Arnoldi iteration for a small matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(1.:9.,6,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed vector determines the first member of the orthonormal basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = randn(6)\n",
    "Q = u/norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication by $\\mathbf{A}$ gives us a new vector in $\\mathcal{K}_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aq = A*Q[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subtract off its projection in the previous direction. The remainder is rescaled to give us the next orthonormal column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Aq - dot(Q[:,1],Aq)*Q[:,1]\n",
    "Q = [Q v/norm(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the next pass, we have to subtract off the projections in two previous directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aq = A*Q[:,2]\n",
    "v = Aq - dot(Q[:,1],Aq)*Q[:,1] - dot(Q[:,2],Aq)*Q[:,2]\n",
    "Q = [Q v/norm(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every step, $Q_m$ is an ONC matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opnorm( Q'*Q - I )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $Q_m$ spans the same space as the 3-dimensional Krylov matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [ u A*u A*A*u ];\n",
    "[Q K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show rank( [Q K] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Q\\K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q*C - K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a triangular matrix with known eigenvalues and a random vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = @. 10 + (1:100)\n",
    "A = triu(rand(100,100),1) + diagm(lambda)\n",
    "b = rand(100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of building up the Krylov matrices, we use the Arnoldi iteration to generate equivalent orthonormal vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,H = arnoldi(A,b,60);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q'Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogres = [norm(Q[:,1:m]'Q[:,1:m] - I) for m = 1:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(0:60, orthogres, m=:o,\n",
    "    xaxis=(L\"m\"),yaxis=(:log10,L\"\\| Q_m^TQ_m - I \\|\"), \n",
    "    title=\"Is Q orthonormal?\",leg=:none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condQ = [cond(Q[:,1:m]) for m = 1:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(0:60, condQ, m=:o,\n",
    "    xaxis=(L\"m\"),yaxis=(:log10,L\"\\kappa(Q_m)\"), \n",
    "    title=\"Is Q well conditioned?\",leg=:none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Arnoldi bases are used to solve the least squares problems defining the GMRES iterates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = [norm(b);zeros(60)]\n",
    "for m = 1:60  \n",
    "    s = [norm(b); zeros(m)]\n",
    "    z = H[1:m+1,1:m]\\s\n",
    "    x = Q[:,1:m]*z\n",
    "    resid[m+1] = norm(b-A*x)\n",
    " end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximations converge smoothly, practically all the way to machine epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(0:60,resid,m=:o,\n",
    "    xaxis=(L\"m\"),yaxis=(:log10,L\"\\| b-Ax_m \\|\"), \n",
    "    title=\"Residual for GMRES\",leg=:none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid[end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiments are based on a matrix resulting from discretization of a partial differential equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 50;\n",
    "A = d^2*matrixdepot(\"poisson\",d)\n",
    "n = size(A,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare unrestarted GMRES with three different thresholds for restarting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterativeSolvers.jl\n",
    "\n",
    "For the following test, we are using the `gmres` function from the [IterativeSolvers.jl](https://github.com/JuliaMath/IterativeSolvers.jl) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods(gmres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gmres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gmres!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ones(n);\n",
    "maxit = 120;  rtol = 1e-8;\n",
    "rest = [maxit,20,40,60]\n",
    "plt = plot([],[],label=\"\",title=\"Convergence of restarted GMRES\",leg=:bottomleft,\n",
    "    xaxis=(\"m\"), yaxis=(:log10,\"residual norm\",[1e-8,100]))\n",
    "for j = 1:4\n",
    "    x,hist = gmres(A,b,restart=rest[j],tol=rtol,maxiter=maxit,log=true)\n",
    "    plot!(hist[:resnorm],label=\"restart = $(rest[j])\")\n",
    "end\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"pure\" curve is the lowest one. All of the other curves agree with it until they encounter their first restart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we compare MINRES and CG on some pseudorandom SPD problems.  The first matrix has a condition number of 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "density = 0.005\n",
    "A = sprandsym(n,density,1e-2)\n",
    "nnz(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cook up a linear system whose solution we happen to know exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1:n)/n\n",
    "b = A*x;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply both methods and compare the convergence of the system residuals, using the built-in function `pcg` in the latter case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xMR,histMR = minres(A,b,tol=1e-12,maxiter=101,log=true)\n",
    "xCG,histCG = cg(A,b,tol=1e-12,maxiter=101,log=true)\n",
    "\n",
    "plot(0:100,[histMR[:resnorm] histCG[:resnorm]]/norm(b),m=:o,label=[\"MINRES\" \"CG\"], \n",
    "    title=\"Convergence of MINRES and CG\",\n",
    "    xaxis=(\"Krylov dimension \\$m\\$\"), yaxis=(:log10,L\"\\|r_m\\| / \\|b\\|\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is virtually no difference between the two methods here when measuring the residual. We see little difference in the errors as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show errorMR = norm( xMR - x ) / norm(x);\n",
    "@show errorCG = norm( xCG - x) / norm(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use a system matrix whose condition number is $10^4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sprandsym(n,density,1e-4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find that the CG residual jumps unexpectedly, but overall both methods converge at about the same linear rate. Note from the scales that both methods have actually made very little progress after 100 iterations, though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xMR,histMR = minres(A,b,tol=1e-12,maxiter=101,log=true)\n",
    "xCG,histCG = cg(A,b,tol=1e-12,maxiter=101,log=true)\n",
    "\n",
    "plot(0:100,[histMR[:resnorm] histCG[:resnorm]]/norm(b),m=:o,label=[\"MINRES\" \"CG\"], \n",
    "    title=\"Convergence of MINRES and CG\",\n",
    "    xaxis=(\"Krylov dimension \\$m\\$\"), yaxis=(:log10,L\"\\|r_m\\| / \\|b\\|\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors confirm that we are nowhere near the correct solution in either case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show errorMR = norm( xMR - x ) / norm(x);\n",
    "@show errorCG = norm( xCG - x) / norm(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a readily available test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = testimage(\"mandrill\");\n",
    "@show m,n = size(img)\n",
    "plot(Gray.(img),title=\"Original image\",aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the one-dimensional tridiagonal blurring matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = @. Float64(Gray(img))\n",
    "\n",
    "B = spdiagm(0=>fill(0.5,m),1=>fill(0.25,m-1),-1=>fill(0.25,m-1));\n",
    "C = spdiagm(0=>fill(0.5,n),1=>fill(0.25,n-1),-1=>fill(0.25,n-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we show the results of using $k=12$ repetitions of the blur in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = X -> B^12 * X * C^12;\n",
    "plot(Gray.(blur(X)),title=\"Blurred image\",aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the earlier process to blur the original image $X$ to get $Z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = testimage(\"mandrill\");\n",
    "m,n = size(img)\n",
    "\n",
    "X = @. Float64(Gray(img))\n",
    "\n",
    "B = spdiagm(0=>fill(0.5,m),1=>fill(0.25,m-1),-1=>fill(0.25,m-1));\n",
    "C = spdiagm(0=>fill(0.5,n),1=>fill(0.25,n-1),-1=>fill(0.25,n-1));\n",
    "blur = X -> B^12 * X * C^12;\n",
    "Z = blur(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we imagine that $X$ is unknown and that the blurred $Z$ is given. We want to invert the blur transformation using the transformation itself. But we have to translate between vectors and images each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unvec = z -> reshape(z,m,n)\n",
    "T = LinearMap(x -> vec(blur(unvec(x))),m*n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply `gmres` to the composite blurring transformation `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = gmres(T,vec(Z),maxiter=50,tol=1e-5);\n",
    "Y = unvec(@.max(0,min(1,y)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Gray.(X),layout=(1,3),subplot=1,title=\"Original\",aspect_ratio=1,size=(900,300))\n",
    "plot!(Gray.(Z),subplot=2,title=\"Blurred image\",aspect_ratio=1)\n",
    "plot!(Gray.(Y),subplot=3,title=\"Deblurred\",aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction isn't perfect because the condition number of repeated blurring happens to be very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a large sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 2.8I + sprand(10000,10000,0.002);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without a preconditioner, GMRES takes a large number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = rand(10000)\n",
    "gmres(A,b,maxiter=300,tol=1e-10,restart=50,log=true);\n",
    "time_plain = @elapsed x,hist1 = gmres(A,b,maxiter=300,tol=1e-10,restart=50,log=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist1[:resnorm],label=\"\", \n",
    "    title=\"GMRES with no preconditioning\",\n",
    "    xaxis=(\"iteration number\"), yaxis=(:log10,\"residual norm\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of incomplete $LU$ factorization drops all sufficiently small values (i.e., replaces them with zeros). This keeps the number of nonzeros in the factors under control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iLU = ilu(A,τ=0.2);\n",
    "@show nnz(A),nnz(iLU.L);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does _not_ produce a true factorization of $A$. However, it's close enough to serve as \"approximate inverse\" in a preconditioner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual preconditioning matrix is $\\mathbf{M}=\\mathbf{L}\\mathbf{U}$. However, we just supply the factorization as a left preconditioner, since the preconditioning step is to solve a system with the matrix $\\mathbf{M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_prec = @elapsed x,hist2 = gmres(A,b,Pl=iLU,maxiter=300,tol=1e-10,restart=50,log=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preconditioning is fairly successful in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist1[:resnorm],label=\"no prec.\", \n",
    "    xaxis=(\"iteration number\"), yaxis=(:log10,\"residual norm\") )\n",
    "plot!(hist2[:resnorm],label=\"iLU prec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably made each GMRES iteration slower because of the need to apply the preconditioner (here, by solving sparse triangular systems). However, there are a lot fewer iterations needed, and there is a modest gain overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a large, sparse, positive definite matrix that arises in the solution of differntial equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "D2 = spdiagm(0=>fill(2,n-1),1=>-ones(n-2),-1=>-ones(n-2))\n",
    "IA = spdiagm(0=>ones(n-1))\n",
    "A = kron(IA,D2) + kron(D2,IA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we solve a linear system with a random right-hand side, without preconditioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = rand(size(A,1))\n",
    "cg(A,b,maxiter=4,tol=1e-10,log=true);  # make timing more accurate\n",
    "time_plain = @elapsed x,hist1 = cg(A,b,maxiter=400,tol=1e-10,log=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist1[:resnorm],label=\"\", \n",
    "    title=\"CG with no preconditioning\",\n",
    "    xaxis=(\"iteration number\"), yaxis=(:log10,\"residual norm\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an SPD matrix we can use an incomplete Cholesky factorization. (It uses a lower triangular $\\mathbf{L}=\\mathbf{R}^T$ rather than an upper triangular $\\mathbf{R}$.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = CholeskyPreconditioner(A,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply CG using this preconditioner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg(A,b,Pl=P,maxiter=4,tol=1e-10,log=true);  # make timing more accurate\n",
    "time_prec = @elapsed x,hist2 = cg(A,b,Pl=P,maxiter=400,tol=1e-10,log=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist1[:resnorm],label=\"no prec.\", \n",
    "    xaxis=(\"iteration number\"), yaxis=(:log10,\"residual norm\"),\n",
    "    title=\"CG with incomplete Cholesky preconditioning\")\n",
    "plot!(hist2[:resnorm],label=\"iChol prec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we got a little improvement with this preconditioner. It saved enough iterations to more than make up for the fact that each iteration now involves extra work."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
